{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###***Accessing Authenticity and Providence of Online Reviews in Travel and Leisure Industry***"
      ],
      "metadata": {
        "id": "nWuONLgc6isd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEGE2eFY6e7t"
      },
      "outputs": [],
      "source": [
        "# Importing the required libraries\n",
        "import string\n",
        "import warnings\n",
        "import re\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from textblob import TextBlob\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading the required pakages\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "9kd5H8_968Lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# specifying the path\n",
        "Real_file = '/content/drive/MyDrive/Colab Notebooks/Real_RedDragonReviews.xlsx'\n",
        "fake_file= '/content/drive/MyDrive/Colab Notebooks/Fake_RedDragonReviews.xlsx'"
      ],
      "metadata": {
        "id": "kKryVSbjmdNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the files\n",
        "df1 = pd.read_excel(Real_file, sheet_name='Sheet1')\n",
        "df2 = pd.read_excel(fake_file, sheet_name='Sheet1')"
      ],
      "metadata": {
        "id": "wMXMBD0C7F4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1"
      ],
      "metadata": {
        "id": "T7kkAGvTE9Xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2"
      ],
      "metadata": {
        "id": "CYTS5w5kWjTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging the two dataframes\n",
        "df_merged= pd.concat([df1, df2], ignore_index=True)"
      ],
      "metadata": {
        "id": "KwjWSN5p-PTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the merged dataframe to a new CSV file\n",
        "df_merged.to_csv('/content/drive/MyDrive/Colab Notebooks/RedDragon_Reviews.csv', index=False)"
      ],
      "metadata": {
        "id": "ogYbtM_c-Q0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the saved file\n",
        "df=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/RedDragon_Reviews.csv')"
      ],
      "metadata": {
        "id": "LelwLBgfB6cB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first five records\n",
        "df.head()"
      ],
      "metadata": {
        "id": "msBw49AqCAFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# last five records\n",
        "df.tail()"
      ],
      "metadata": {
        "id": "TzyMnxDJCB9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Data Pre-Processing***"
      ],
      "metadata": {
        "id": "8A7WdZu3J2Co"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# no.of rows and columns\n",
        "df.shape"
      ],
      "metadata": {
        "id": "-rmKuxNCCE5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# displaying the information about the dataset\n",
        "print(\"Basic information on the dataset:\")\n",
        "print(df.info())"
      ],
      "metadata": {
        "id": "fkLb6e-BCJRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicates\n",
        "df = df.drop_duplicates()"
      ],
      "metadata": {
        "id": "fUgKvZqyCKb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking for the missing values\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "JAPnL9NaCM7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filling the missing values for values\n",
        "mode_value = df['value'].mode()[0]\n",
        "rounded_mode_value = round(mode_value)\n",
        "df['value'] = df['value'].fillna(rounded_mode_value)"
      ],
      "metadata": {
        "id": "TwNr8Nt2CR9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filling the missing values for service\n",
        "mode_value = df['service'].mode()[0]\n",
        "rounded_mode_value = round(mode_value)\n",
        "df['service'] = df['service'].fillna(rounded_mode_value)"
      ],
      "metadata": {
        "id": "CydVNnfHCTSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filling the missing values for location\n",
        "mode_value = df['location'].mode()[0]\n",
        "rounded_mode_value = round(mode_value)\n",
        "df['location'] = df['location'].fillna(rounded_mode_value)"
      ],
      "metadata": {
        "id": "l5t6B5EACU94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filling the missing values for rooms\n",
        "mode_value = df['rooms'].mode()[0]\n",
        "rounded_mode_value = round(mode_value)\n",
        "df['rooms'] = df['rooms'].fillna(rounded_mode_value)"
      ],
      "metadata": {
        "id": "77qLBG59CWyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filling the missing values for cleanliness\n",
        "mode_value = df['cleanliness'].mode()[0]\n",
        "rounded_mode_value = round(mode_value)\n",
        "df['cleanliness'] = df['cleanliness'].fillna(rounded_mode_value)"
      ],
      "metadata": {
        "id": "CPf8MCgyCYvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filling the missing values for sleep_quality\n",
        "mode_value = df['sleep_quality'].mode()[0]\n",
        "rounded_mode_value = round(mode_value)\n",
        "df['sleep_quality'] = df['sleep_quality'].fillna(rounded_mode_value)"
      ],
      "metadata": {
        "id": "r_qOL2bICcZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a new coloumn label_encoded with values for 'OR'=0 and 'AI'=1\n",
        "df['label_encoded'] = df['label'].apply(lambda x: 1 if x == 'AI' else 0)\n",
        "df['label_encoded']"
      ],
      "metadata": {
        "id": "ppzaCVCt7fkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize stop words, stemmer, and lemmatizer for text preprocessing\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.update([\n",
        "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your',\n",
        "    'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it',\n",
        "    \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this',\n",
        "    'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',\n",
        "    'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
        "    'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n",
        "    'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\n",
        "    'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most',\n",
        "    'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can',\n",
        "    'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren',\n",
        "    \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven',\n",
        "    \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\",\n",
        "    'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"\n",
        "])\n",
        "\n",
        "# Initialize stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "2kok5zxv7grD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # function to preprocess the text\n",
        "# def preprocess_text(text):\n",
        "#     text = text.lower()  #converting the text to lower case\n",
        "#     text = text.translate(str.maketrans('', '', string.punctuation)) #Removing the punctuation\n",
        "#     text = re.sub(r'\\d+', '', text)  # Removing numbers\n",
        "#     text = re.sub(r\"\\W\", \" \", text)         # Replace non-word characters with a space\n",
        "#     text = re.sub(r\"\\d\", \" \", text)         # Replace digits with a space\n",
        "#     text = re.sub(r\"\\s+[a-z]\\s+\", \" \", text, flags=re.I)  # Replace single letters with a space\n",
        "#     text = re.sub(r\"\\s+\", \" \", text)        # Replace multiple spaces with a single space\n",
        "#     text = re.sub(r\"^\\s\", \"\", text)         # Remove leading whitespace\n",
        "#     text = re.sub(r\"\\s$\", \"\", text)         # Remove trailing whitespace\n",
        "#     words = word_tokenize(text) #Tokenizing the text\n",
        "#     words = [word for word in words if word not in stop_words] # removeing the stop words\n",
        "#     return ' '.join(words)"
      ],
      "metadata": {
        "id": "A5dYrfu3828J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Apply the preprocessing to the review column\n",
        "# df['processed_review'] = df['review'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "RCvzad049v-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df['processed_review']"
      ],
      "metadata": {
        "id": "-PPfD_zn90k9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Stemming***"
      ],
      "metadata": {
        "id": "Krqg_4CmHZPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Remove numbers from the text\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "    # Remove punctuation from text\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "    # Remove stopwords and apply stemming to the text\n",
        "    words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
        "    return ' '.join(words)"
      ],
      "metadata": {
        "id": "FFdpcN-Yni-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the preprocessing to the review column\n",
        "df['processed_review'] = df['review'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "jnZ_qzTYnjzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['review','processed_review']]"
      ],
      "metadata": {
        "id": "cLpR275knl0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Lemmatization**"
      ],
      "metadata": {
        "id": "0WiaqOXIIJK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # remove numbers from the text\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "    # remove punctuation from the text\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "    # Remove stopwords and lemmatize the words\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    return ' '.join(words)"
      ],
      "metadata": {
        "id": "GJKBrUbtntB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the preprocessing to the review column\n",
        "df['processed_review'] = df['review'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "3tfrLQeanvVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['review','processed_review']]"
      ],
      "metadata": {
        "id": "b0y5Sb8Rnw4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def lemmatize_text(text):\n",
        "#     words = word_tokenize(text)\n",
        "#     lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "#     return ' '.join(lemmatized_words)"
      ],
      "metadata": {
        "id": "3arD7E39lmBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Apply the preprocessing to the review column\n",
        "# df['processed_review'] = df['processed_review'].apply(lemmatize_text)"
      ],
      "metadata": {
        "id": "7iTdzpiTISIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df['processed_review'].head()"
      ],
      "metadata": {
        "id": "7emS5DClIhOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df['processed_review'].tail()"
      ],
      "metadata": {
        "id": "Suc19tlHIiLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Function to perform POS tagging\n",
        "# def pos_tagging(text):\n",
        "#     words = word_tokenize(text)\n",
        "#     return pos_tag(words)"
      ],
      "metadata": {
        "id": "IV1jCtXCI9wR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply POS tagging to the processed reviews\n",
        "#df['pos_tags'] = df['processed_review'].apply(pos_tagging)"
      ],
      "metadata": {
        "id": "Ghp-GUdUJGnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df['pos_tags']"
      ],
      "metadata": {
        "id": "A1SQ0Nt7JL3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of words in sentence\n",
        "sent_len = [] #creating an empty list\n",
        "for sent in df['processed_review']:\n",
        "  sent_len.append(len(word_tokenize(sent)))\n",
        "df['sent_len'] = sent_len\n",
        "df['num_chars'] = df['processed_review'].apply(len)# Number of characters\n",
        "df['avg_word_len'] = df['processed_review'].apply(lambda x: sum(len(word) for word in x.split()) / len(x.split()))# Average word length\n",
        "df['num_unique_words'] = df['processed_review'].apply(lambda x: len(set(x.split())))# Number of unique words"
      ],
      "metadata": {
        "id": "gYDR45KI2e87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of nouns, verbs, and adjectives\n",
        "def count_pos(text):\n",
        "    words = word_tokenize(text)\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "    pos_counts = Counter(tag for word, tag in pos_tags)\n",
        "    return pos_counts['NN'], pos_counts['VB'], pos_counts['JJ']  # Nouns, Verbs, Adjectives"
      ],
      "metadata": {
        "id": "YF4KA3w2JMaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the function and get a list of tuples\n",
        "pos_counts = df['processed_review'].apply(count_pos)\n",
        "\n",
        "# Convert the list of tuples into a DataFrame\n",
        "pos_df = pd.DataFrame(pos_counts.tolist(), columns=['num_nouns', 'num_verbs', 'num_adjs'])\n",
        "\n",
        "# Concatenate the original DataFrame with the new POS DataFrame\n",
        "df = pd.concat([df, pos_df], axis=1)"
      ],
      "metadata": {
        "id": "IMt7A3ruERed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['sent_len', 'num_chars','avg_word_len','num_unique_words','num_nouns','num_nouns','num_verbs','num_adjs']]"
      ],
      "metadata": {
        "id": "kMROiypV2xcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Exploratory Data Analysis***"
      ],
      "metadata": {
        "id": "baJ8Hh3D7WkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count plot of Rating by OR v/s AI\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(data=df, x='rating', hue='label', palette='flare')\n",
        "plt.title('Count Plot of Overall Ratings Provided by OR v/s AI',fontweight='bold',fontstyle='italic')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Count')\n",
        "plt.legend(title='Label', loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7opi74fuCzcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the data based on label, for 'OR'\n",
        "df_filtered_or = df[df['label'] == 'OR']\n",
        "\n",
        "# Conversion of the DataFrame to long format for 'OR'\n",
        "df_or = df_filtered_or.melt(id_vars=['label'], value_vars=['value', 'service', 'location', 'rooms', 'cleanliness', 'sleep_quality'],\n",
        "                                   var_name='category', value_name='rating_value')\n",
        "\n",
        "# Plot the data for 'OR'\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(data=df_or, x='category', hue='rating_value', palette='colorblind')\n",
        "plt.title('Count Plot of Ratings for Different Sectors Provided by OR',fontweight='bold',fontstyle='italic')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Count')\n",
        "plt.legend(title='Rating', loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0soDv5aFJeTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the data based on label, for 'AI'\n",
        "df_filtered_ai = df[df['label'] == 'AI']\n",
        "\n",
        "# conversion of the DataFrame to long format for 'AI'\n",
        "df_ai = df_filtered_ai.melt(id_vars=['label'], value_vars=['value', 'service', 'location', 'rooms', 'cleanliness', 'sleep_quality'],\n",
        "                                   var_name='category', value_name='rating_value')\n",
        "\n",
        "# Plot the data for 'AI'\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(data=df_ai, x='category', hue='rating_value', palette='viridis')\n",
        "plt.title('Count Plot of Ratings for Different Categories Provided by AI',fontweight='bold',fontstyle='italic')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Count')\n",
        "plt.legend(title='Rating', loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uaUhl5VOJ61z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot to check if the dataset is balanced based on the tareget varaibale'label' column\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(data=df, x='label', palette='viridis')\n",
        "plt.title('Count Plot to Check distribution of the target variable (label)',fontweight='bold',fontstyle='italic')\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DHJlMRM9FtbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# overall distribution of rating percentage for both \"OR\" & \"AI\"\n",
        "plt.pie(df['rating'].value_counts(), labels=df['rating'].unique().tolist(), autopct='%1.1f%%')\n",
        "plt.title('Overall Rating Distribution in percentage',fontweight='bold',fontstyle='italic')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ECL7qLenI5Ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Sentiment Analysis***"
      ],
      "metadata": {
        "id": "Ye20yvG4yLHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***TextBlob***"
      ],
      "metadata": {
        "id": "PquyD6goFIjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#installing the TextBlob library\n",
        "!pip install textblob"
      ],
      "metadata": {
        "id": "3k_RMJ_dxzD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the library\n",
        "from textblob import TextBlob"
      ],
      "metadata": {
        "id": "NMb3hRDsyVB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get  polarity\n",
        "def get_polarity(text):\n",
        "    polarity = TextBlob(text).sentiment.polarity\n",
        "    return int(round(polarity))\n",
        "\n",
        "# Function to get subjectivity\n",
        "def get_subjectivity(text):\n",
        "    subjectivity = TextBlob(text).sentiment.subjectivity\n",
        "    return int(round(subjectivity))"
      ],
      "metadata": {
        "id": "nO6y8aCey113"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['textblob_polarity'] = df['processed_review'].apply(get_polarity)\n",
        "df['textblob_subjectivity'] = df['processed_review'].apply(get_subjectivity)"
      ],
      "metadata": {
        "id": "KPlIttrXy23O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['label','textblob_polarity','textblob_subjectivity']]"
      ],
      "metadata": {
        "id": "RyLQyv2WzPum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualization of Polarity and subjectivity**"
      ],
      "metadata": {
        "id": "O9qjMd-o0b-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the DataFrame for 'AI' and 'OR' reviews\n",
        "df_ai = df[df['label'] == 'AI']\n",
        "df_or = df[df['label'] == 'OR']\n",
        "\n",
        "# Create a figure with 2 rows and 2 columns of subplots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Plotting Polarity for OR\n",
        "sns.countplot(x='textblob_polarity', data=df_or, palette='magma', ax=axes[0, 0])\n",
        "axes[0, 0].set_title('Polarity Distribution for OR Reviews',fontweight='bold',fontstyle='italic')\n",
        "axes[0, 0].set_xlabel('Polarity')\n",
        "axes[0, 0].set_ylabel('Count')\n",
        "\n",
        "# Plotting Polarity for AI\n",
        "sns.countplot(x='textblob_polarity', data=df_ai, palette='viridis', ax=axes[0, 1])\n",
        "axes[0, 1].set_title('Polarity Distribution for AI Reviews',fontweight='bold',fontstyle='italic')\n",
        "axes[0, 1].set_xlabel('Polarity')\n",
        "axes[0, 1].set_ylabel('Count')\n",
        "\n",
        "# Plotting polarity for OR\n",
        "sns.countplot(x='textblob_subjectivity', data=df_or, palette='bright', ax=axes[1, 0])\n",
        "axes[1, 0].set_title('Subjectivity Distribution for OR Reviews',fontweight='bold',fontstyle='italic')\n",
        "axes[1, 0].set_xlabel('Subjectivity')\n",
        "axes[1, 0].set_ylabel('Count')\n",
        "\n",
        "# Plotting polarity for AI\n",
        "sns.countplot(x='textblob_subjectivity', data=df_ai, palette='Greens', ax=axes[1, 1])\n",
        "axes[1, 1].set_title(' Subjectivity Distribution for AI Reviews',fontweight='bold',fontstyle='italic')\n",
        "axes[1, 1].set_xlabel('Subjectivity')\n",
        "axes[1, 1].set_ylabel('Count')\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3p6OVCr1Zt5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***VADER (Valence Aware Dictionary and sEntiment Reasoner)***"
      ],
      "metadata": {
        "id": "4co9yyk32hNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation of VADER Sentiment library\n",
        "!pip install vaderSentiment"
      ],
      "metadata": {
        "id": "l1HwcngD2cCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the VADER sentiment analyzer\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "analyzer = SentimentIntensityAnalyzer()"
      ],
      "metadata": {
        "id": "vhNKzrw77Wux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get sentiment scores and labels\n",
        "def get_vader_analysis(text):\n",
        "    sentiment_dict = analyzer.polarity_scores(text)\n",
        "    sentiment_label = 'positive' if sentiment_dict['compound'] >= 0.05 else ('negative' if sentiment_dict['compound'] <= -0.05 else 'neutral')\n",
        "    return pd.Series([sentiment_dict['neg'], sentiment_dict['neu'], sentiment_dict['pos'], sentiment_dict['compound'], sentiment_label])"
      ],
      "metadata": {
        "id": "h4jGfTqR7Z2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply VADER sentiment analysis to the processed reviews\n",
        "df[['vader_neg', 'vader_neu', 'vader_pos', 'compound', 'vader_sentiment_label']] = df['processed_review'].apply(get_vader_analysis)"
      ],
      "metadata": {
        "id": "jDYAnpew7edS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the VADER sentiment scores and labels\n",
        "df[['processed_review', 'vader_neg', 'vader_neu', 'vader_pos', 'compound', 'vader_sentiment_label']]"
      ],
      "metadata": {
        "id": "47-tU3yk-XzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count plot of VADER sentiment labels by OR vs. AI\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(data=df, x='vader_sentiment_label', hue='label', palette='viridis')\n",
        "plt.title('VADER Sentiment Distribution for OR vs. AI Reviews', fontweight='bold', fontstyle='italic')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Count')\n",
        "plt.legend(title='Label', loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G_yc42aE-aM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***RQ2: â€œHow does the sentiments of reviews varies between genuine and fake reviews?***"
      ],
      "metadata": {
        "id": "7KZaWKN8-LQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "#scipy used to perform independent t-test\n",
        "from scipy.stats import ttest_ind"
      ],
      "metadata": {
        "id": "G77WLKPI8hUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the reviews based on 'OR' (genuine) and 'AI' (fake) labels\n",
        "Original_reviews = df[df['label'] == 'OR']['compound']\n",
        "AI_reviews = df[df['label'] == 'AI']['compound']"
      ],
      "metadata": {
        "id": "vn7qGYuF-UbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descriptive Statistics\n",
        "print(f\"Genuine Reviews - Mean: {Original_reviews.mean()}, Std Dev: {Original_reviews.std()}\")\n",
        "print(f\"AI Reviews - Mean: {AI_reviews.mean()}, Std Dev: {AI_reviews.std()}\")"
      ],
      "metadata": {
        "id": "LncYxiDo_M5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform t-test\n",
        "t_statistic, p_value = ttest_ind(Original_reviews, AI_reviews)\n",
        "print(f\"T-statistic: {t_statistic}, P-value: {p_value}\")"
      ],
      "metadata": {
        "id": "yVLn5wVfFkhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization: Boxplot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.boxplot([Original_reviews, AI_reviews], labels=['Original Reviews', 'AI Reviews'])\n",
        "plt.title('Sentiment Score Distribution',fontweight='bold',fontstyle='italic')\n",
        "plt.ylabel('Compound Sentiment Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1CscwWM5_lNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.kdeplot(Original_reviews, shade=True, label='Original Reviews', color='blue')\n",
        "sns.kdeplot(AI_reviews, shade=True, label='AI Reviews', color='red')\n",
        "plt.title('Density Plot of Sentiment Scores', fontweight='bold', fontstyle='italic')\n",
        "plt.xlabel('Compound Sentiment Score')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Yw5iO1ZtEmb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***RQ3: What do the word patterns shown in the WordClouds reveals the opinions between fake and genuine reviews?***"
      ],
      "metadata": {
        "id": "6LhD4ohw373a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***WordCloud Visualization***"
      ],
      "metadata": {
        "id": "11PdxAX67aG9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***VADER***"
      ],
      "metadata": {
        "id": "41rL2W71kWdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from wordcloud import WordCloud"
      ],
      "metadata": {
        "id": "lWBii6eRYbwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate word cloud for postive review\n",
        "def generate_wordcloud(text, ax, title, background_color):\n",
        "    wordcloud = WordCloud(width=800, height=800, background_color=background_color, colormap='viridis', min_font_size=10).generate(text)\n",
        "    ax.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(title, fontsize=16)  # Reduced font size\n",
        "\n",
        "# Create figure and axes for positive reviews\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=[10, 5], facecolor='white')  # Adjusted figure size and facecolor\n",
        "\n",
        "# Positive Reviews\n",
        "text_pos_or = ' '.join(df[(df['label'] == 'OR') & (df['vader_sentiment_label'] == 'positive')]['processed_review'])\n",
        "generate_wordcloud(text_pos_or, ax1, 'OR - Positive Reviews', background_color='white')\n",
        "\n",
        "text_pos_ai = ' '.join(df[(df['label'] == 'AI') & (df['vader_sentiment_label'] == 'positive')]['processed_review'])\n",
        "generate_wordcloud(text_pos_ai, ax2, 'AI - Positive Reviews', background_color='black')\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wFGoS4VPYk27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate word cloud for negative reviews\n",
        "def generate_wordcloud(text, ax, title, background_color):\n",
        "    wordcloud = WordCloud(width=800, height=800, background_color=background_color, colormap='cool', min_font_size=10).generate(text)\n",
        "    ax.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(title, fontsize=16)  # Reduced font size\n",
        "\n",
        "# Create figure and axes for negative reviews\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=[10, 5], facecolor='white')  # Adjusted figure size and facecolor\n",
        "\n",
        "# Negative Reviews\n",
        "text_neg_or = ' '.join(df[(df['label'] == 'OR') & (df['vader_sentiment_label'] == 'negative')]['processed_review'])\n",
        "generate_wordcloud(text_neg_or, ax1, 'OR - Negative Reviews', background_color='lightblue')\n",
        "\n",
        "text_neg_ai = ' '.join(df[(df['label'] == 'AI') & (df['vader_sentiment_label'] == 'negative')]['processed_review'])\n",
        "generate_wordcloud(text_neg_ai, ax2, 'AI - Negative Reviews', background_color='lightpink')\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ySUVUfvPeKs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate word cloud for neutral reviews\n",
        "def generate_wordcloud(text, ax, title, background_color):\n",
        "    wordcloud = WordCloud(width=800, height=800, background_color=background_color, colormap='plasma', min_font_size=10).generate(text)\n",
        "    ax.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(title, fontsize=16)  # Reduced font size\n",
        "\n",
        "# Create figure and axes for neutral reviews\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=[10, 5], facecolor='white')  # Adjusted figure size and facecolor\n",
        "\n",
        "# Neutral Reviews\n",
        "text_neu_or = ' '.join(df[(df['label'] == 'OR') & (df['vader_sentiment_label'] == 'neutral')]['processed_review'])\n",
        "generate_wordcloud(text_neu_or, ax1, 'OR - Neutral Reviews', background_color='green')\n",
        "\n",
        "text_neu_ai = ' '.join(df[(df['label'] == 'AI') & (df['vader_sentiment_label'] == 'neutral')]['processed_review'])\n",
        "generate_wordcloud(text_neu_ai, ax2, 'AI - Neutral Reviews', background_color='gray')\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6ufIEJsD87KA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Feature Engineering***"
      ],
      "metadata": {
        "id": "LLO-ftv7opMM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Unigram***"
      ],
      "metadata": {
        "id": "1S8L9ceAGpHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the processed reviews using unigrams\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
        "X = vectorizer.fit_transform(df['processed_review'])"
      ],
      "metadata": {
        "id": "TIrMWgt11wtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the unigrams into a DataFrame\n",
        "unigram_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "gdbojLFu11ti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the unigram features with the original DataFrame\n",
        "df_unigrams = pd.concat([df, unigram_df], axis=1)"
      ],
      "metadata": {
        "id": "pTEaDQwT146P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that we only have numeric columns in unigram_features by filtering explicitly\n",
        "unigram_features = unigram_df.columns.tolist()"
      ],
      "metadata": {
        "id": "fgR8A3GP1_1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out only numeric columns for processing\n",
        "numeric_unigram_features = [col for col in unigram_features if pd.api.types.is_numeric_dtype(df_unigrams[col])]"
      ],
      "metadata": {
        "id": "xeNZ8PIu8Pf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the sum of each unigram for AI reviews\n",
        "unigram_ai_sums = df_unigrams[df_unigrams['label_encoded'] == 1][numeric_unigram_features].sum().sort_values(ascending=False)\n",
        "\n",
        "# Calculate the sum of each unigram for OR reviews\n",
        "unigram_or_sums = df_unigrams[df_unigrams['label_encoded'] == 0][numeric_unigram_features].sum().sort_values(ascending=False)\n",
        "\n",
        "# Select the top 20 unigrams for OR reviews\n",
        "top_n_unigrams_or = unigram_or_sums.head(20)\n",
        "\n",
        "# Select the top 20 unigrams for OR reviews\n",
        "top_n_unigrams_ai = unigram_ai_sums.head(20)"
      ],
      "metadata": {
        "id": "i39fghiA8axx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust threshold if necessary\n",
        "threshold = top_n_unigrams_or.min() + 1\n",
        "\n",
        "# Filter out unigrams with low frequencies (set threshold as needed)\n",
        "top_n_unigrams_or_filtered = top_n_unigrams_or[top_n_unigrams_or > threshold]"
      ],
      "metadata": {
        "id": "OSZ_Ccbj8fM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the filtered top 20 unigrams for OR reviews\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x=top_n_unigrams_or_filtered.values, y=top_n_unigrams_or_filtered.index, palette=\"viridis\")\n",
        "plt.title('Top 20 Most Frequent Unigrams in OR Reviews',fontweight='bold', fontstyle='italic')\n",
        "plt.xlabel('Frequency')\n",
        "plt.ylabel('Unigrams')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4gKLjPue8jO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust threshold if necessary\n",
        "threshold = top_n_unigrams_ai.min() + 1\n",
        "\n",
        "# Filter out unigrams with low frequencies (set threshold as needed)\n",
        "top_n_unigrams_ai_filtered = top_n_unigrams_ai[top_n_unigrams_ai > threshold]"
      ],
      "metadata": {
        "id": "OcVvOi1X9xkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the filtered top 20 unigrams for OR reviews\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x=top_n_unigrams_ai_filtered.values, y=top_n_unigrams_ai_filtered.index, palette=\"cividis\")\n",
        "plt.title('Top 20 Most Frequent Unigrams in AI Reviews',fontweight='bold', fontstyle='italic')\n",
        "plt.xlabel('Frequency')\n",
        "plt.ylabel('Unigrams')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2sra9reP9yIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Bigram***"
      ],
      "metadata": {
        "id": "WF4vo5Qw1pKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the processed reviews using bigrams\n",
        "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
        "X = vectorizer.fit_transform(df['processed_review'])"
      ],
      "metadata": {
        "id": "Ftrd9r0ZotFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the bigrams into a DataFrame\n",
        "bigram_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "oH39KqdZozRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the bigram features with the original DataFrame\n",
        "df_bigrams = pd.concat([df, bigram_df], axis=1)"
      ],
      "metadata": {
        "id": "GQ2J5Yq0pX0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the feature columns, including bigram features\n",
        "bigram_features = bigram_df.columns.tolist()"
      ],
      "metadata": {
        "id": "dmTE2CNsuKHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the sum of each bigram for AI reviews\n",
        "bigram_ai_sums = df_bigrams[df_bigrams['label_encoded'] == 1][bigram_df.columns].sum().sort_values(ascending=False)\n",
        "\n",
        "# Calculate the sum of each bigram for OR reviews\n",
        "bigram_or_sums = df_bigrams[df_bigrams['label_encoded'] == 0][bigram_df.columns].sum().sort_values(ascending=False)\n",
        "\n",
        "# Select the top 20 bigrams for AI reviews\n",
        "top_n_bigrams_ai = bigram_ai_sums.head(20)\n",
        "\n",
        "# Select the top 20 bigrams for OR reviews\n",
        "top_n_bigrams_or = bigram_or_sums.head(20)"
      ],
      "metadata": {
        "id": "Lpa61DmzpDlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the top 20 bigrams for OR reviews\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x=top_n_bigrams_or.values, y=top_n_bigrams_or.index, palette=\"plasma\")\n",
        "plt.title('Top 20 Most Frequent Bigrams in OR Reviews',fontweight='bold', fontstyle='italic')\n",
        "plt.xlabel('Frequency')\n",
        "plt.ylabel('Bigrams')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g8-rPzFopM4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the top 20 bigrams for AI reviews\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x=top_n_bigrams_ai.values, y=top_n_bigrams_ai.index, palette=\"inferno\")\n",
        "plt.title('Top 20 Most Frequent Bigrams in AI Reviews',fontweight='bold', fontstyle='italic')\n",
        "plt.xlabel('Frequency')\n",
        "plt.ylabel('Bigrams')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lvltzTcMpLAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Trigram***"
      ],
      "metadata": {
        "id": "W369shGgu_sD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the processed reviews using trigrams\n",
        "vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
        "x = vectorizer.fit_transform(df['processed_review'])"
      ],
      "metadata": {
        "id": "FW9ieHjKs-k-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the trigrams into a DataFrame\n",
        "trigram_df = pd.DataFrame(x.toarray(), columns=vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "ohqm2Bteuasa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trigram_df"
      ],
      "metadata": {
        "id": "wmXiQOpyvdlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the trigram features with the original DataFrame\n",
        "df_trigrams = pd.concat([df, trigram_df], axis=1)"
      ],
      "metadata": {
        "id": "DwIokh9Oue6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the feature columns, including trigram features\n",
        "#trigram_features = trigram_df.columns.tolist()"
      ],
      "metadata": {
        "id": "73ufWB1yukuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the sum of each trigram for AI reviews\n",
        "trigram_ai_sums = df_trigrams[df_trigrams['label_encoded'] == 1][trigram_df.columns].sum().sort_values(ascending=False)\n",
        "\n",
        "# Calculate the sum of each trigram for OR reviews\n",
        "trigram_or_sums = df_trigrams[df_trigrams['label_encoded'] == 0][trigram_df.columns].sum().sort_values(ascending=False)\n",
        "\n",
        "# Select the top 10 trigrams for AI reviews\n",
        "top_n_trigrams_ai = trigram_ai_sums.head(20)\n",
        "\n",
        "# Select the top 10 trigrams for OR reviews\n",
        "top_n_trigrams_or = trigram_or_sums.head(20)"
      ],
      "metadata": {
        "id": "NTqw0LInurep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the top 10 trigrams for OR reviews\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.barplot(x=top_n_trigrams_or.values, y=top_n_trigrams_or.index, palette=\"Accent\")\n",
        "plt.title('Top 20 Most Frequent Trigrams in OR Reviews',fontweight='bold',fontstyle='italic')\n",
        "plt.xlabel('Frequency')\n",
        "plt.ylabel('Trigrams')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kHMUqX5Su-YW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the top 20 trigrams for AI reviews\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.barplot(x=top_n_trigrams_ai.values, y=top_n_trigrams_ai.index, palette=\"plasma\")\n",
        "plt.title('Top 20 Most Frequent Trigrams in AI Reviews',fontweight='bold',fontstyle='italic')\n",
        "plt.xlabel('Frequency')\n",
        "plt.ylabel('Trigrams')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pTjeQjZbv2ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***PCA***"
      ],
      "metadata": {
        "id": "rkeYVcr_0AtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardizing the trigram features\n",
        "trigram_features_standardized = StandardScaler().fit_transform(trigram_df)"
      ],
      "metadata": {
        "id": "3gEn1n2JwzAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply PCA to retain 95% of variance\n",
        "pca = PCA(n_components=0.95)\n",
        "trigram_pca = pca.fit_transform(trigram_features_standardized)"
      ],
      "metadata": {
        "id": "9FpMpkWfw0QB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the PCA result to a DataFrame\n",
        "trigram_pca_df = pd.DataFrame(trigram_pca, columns=[f'pca_{i+1}' for i in range(trigram_pca.shape[1])])"
      ],
      "metadata": {
        "id": "hJdG_Sn5w1Zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the PCA components with the original DataFrame\n",
        "df = pd.concat([df_trigrams, trigram_pca_df], axis=1)"
      ],
      "metadata": {
        "id": "kdemA_Bnl-eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca_features = trigram_pca_df.columns.tolist()"
      ],
      "metadata": {
        "id": "svZXHOJ-mBrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trigram_pca_df"
      ],
      "metadata": {
        "id": "VvO4LgeEYRnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Machine Learning (ML)***"
      ],
      "metadata": {
        "id": "YdYj6i88QXuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# feature columns\n",
        "features = ['rating', 'value', 'service', 'location', 'rooms', 'cleanliness', 'sleep_quality',\n",
        "            'sent_len', 'num_chars', 'avg_word_len', 'num_unique_words', 'num_nouns', 'num_verbs',\n",
        "            'num_adjs','vader_neg', 'vader_neu', 'vader_pos', 'compound','textblob_polarity','textblob_subjectivity']+ pca_features"
      ],
      "metadata": {
        "id": "xtDOYdBJpjUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = 'label_encoded'"
      ],
      "metadata": {
        "id": "Oxv-Z1rbwOWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[features]\n",
        "Y = df[target]"
      ],
      "metadata": {
        "id": "rGK18yojmnv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training (40%), testing (30%), and validation (30%) sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, Y, test_size=0.6, random_state=42, stratify=Y)\n",
        "X_test, X_valid, y_test, y_valid = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "\n",
        "# Print dataset sizes\n",
        "print(f\"Training set size: {X_train.shape[0]}\")\n",
        "print(f\"Testing set size: {X_test.shape[0]}\")\n",
        "print(f\"Validation set size: {X_valid.shape[0]}\")"
      ],
      "metadata": {
        "id": "WzMFqjGQprNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Logistic Regression model\n",
        "lr_model = LogisticRegression(random_state=42, max_iter=10000)\n",
        "lr_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "nLKZK9vNps6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the test set\n",
        "y_test_pred = lr_model.predict(X_test)\n",
        "print(\"Logistic Regression:\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "print(\"Test Set Evaluation:\\n\")\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred)}\\n\")\n",
        "print(f\"Precision: {precision_score(y_test, y_test_pred, average='weighted')}\\n\")\n",
        "print(f\"Recall: {recall_score(y_test, y_test_pred, average='weighted')}\\n\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_test_pred, average='weighted')}\\n\")\n",
        "\n",
        "# Confusion Matrix for Test Set\n",
        "conf_matrix_test = confusion_matrix(y_test, y_test_pred)\n",
        "print(\"Confusion Matrix - Test Set:\")\n",
        "print(conf_matrix_test, \"\\n\")\n",
        "\n",
        "# Predict on the validation set\n",
        "y_valid_pred = lr_model.predict(X_valid)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "print(\"Validation Set Evaluation:\\n\")\n",
        "print(classification_report(y_valid, y_valid_pred))\n",
        "print(f\"Accuracy: {accuracy_score(y_valid, y_valid_pred)}\\n\")\n",
        "print(f\"Precision: {precision_score(y_valid, y_valid_pred, average='weighted')}\\n\")\n",
        "print(f\"Recall: {recall_score(y_valid, y_valid_pred, average='weighted')}\\n\")\n",
        "print(f\"F1 Score: {f1_score(y_valid, y_valid_pred, average='weighted')}\\n\")\n",
        "\n",
        "# Confusion Matrix for Validation Set\n",
        "conf_matrix_valid = confusion_matrix(y_valid, y_valid_pred)\n",
        "print(\"Confusion Matrix - Validation Set:\")\n",
        "print(conf_matrix_valid)"
      ],
      "metadata": {
        "id": "H8QbglYc9Eqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Decision Tree model\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "fswd6H6ZqBiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the test set\n",
        "y_test_pred = dt_model.predict(X_test)\n",
        "print(\"Decision Tree:\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "print(\"Test Set Evaluation:\\n\")\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred)}\\n\")\n",
        "print(f\"Precision: {precision_score(y_test, y_test_pred, average='weighted')}\\n\")\n",
        "print(f\"Recall: {recall_score(y_test, y_test_pred, average='weighted')}\\n\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_test_pred, average='weighted')}\\n\")\n",
        "\n",
        "# Confusion Matrix for Test Set\n",
        "conf_matrix_test = confusion_matrix(y_test, y_test_pred)\n",
        "print(\"Confusion Matrix - Test Set:\")\n",
        "print(conf_matrix_test, \"\\n\")\n",
        "\n",
        "# Predict on the validation set\n",
        "y_valid_pred = dt_model.predict(X_valid)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "print(\"Validation Set Evaluation:\\n\")\n",
        "print(classification_report(y_valid, y_valid_pred))\n",
        "print(f\"Accuracy: {accuracy_score(y_valid, y_valid_pred)}\\n\")\n",
        "print(f\"Precision: {precision_score(y_valid, y_valid_pred, average='weighted')}\\n\")\n",
        "print(f\"Recall: {recall_score(y_valid, y_valid_pred, average='weighted')}\\n\")\n",
        "print(f\"F1 Score: {f1_score(y_valid, y_valid_pred, average='weighted')}\\n\")\n",
        "\n",
        "# Confusion Matrix for Validation Set\n",
        "conf_matrix_valid = confusion_matrix(y_valid, y_valid_pred)\n",
        "print(\"Confusion Matrix - Validation Set:\")\n",
        "print(conf_matrix_valid)"
      ],
      "metadata": {
        "id": "qP1muG2n-J60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf_matrix_valid = confusion_matrix(y_valid, y_valid_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix_valid, annot=True, fmt='d', cmap='terrain', xticklabels=['OR', 'AI'], yticklabels=['OR', 'AI'])\n",
        "plt.title(\"Confusion Matrix\",fontweight='bold',fontstyle='italic')\n",
        "plt.xlabel(\"Predicted Label\",fontweight='bold',fontstyle='italic')\n",
        "plt.ylabel(\"True Label\",fontweight='bold',fontstyle='italic')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xDS8Kus9J8tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svc_model = SVC(random_state=42)\n",
        "svc_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "D7ZkAwyDqLwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the test set\n",
        "y_test_pred = svc_model.predict(X_test)\n",
        "print(\"SVM:\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "print(\"Test Set Evaluation:\\n\")\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred)}\\n\")\n",
        "print(f\"Precision: {precision_score(y_test, y_test_pred, average='weighted')}\\n\")\n",
        "print(f\"Recall: {recall_score(y_test, y_test_pred, average='weighted')}\\n\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_test_pred, average='weighted')}\\n\")\n",
        "\n",
        "# Confusion Matrix for Test Set\n",
        "conf_matrix_test = confusion_matrix(y_test, y_test_pred)\n",
        "print(\"Confusion Matrix - Test Set:\")\n",
        "print(conf_matrix_test, \"\\n\")\n",
        "\n",
        "# Predict on the validation set\n",
        "y_valid_pred = svc_model.predict(X_valid)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "print(\"Validation Set Evaluation:\\n\")\n",
        "print(classification_report(y_valid, y_valid_pred))\n",
        "print(f\"Accuracy: {accuracy_score(y_valid, y_valid_pred)}\\n\")\n",
        "print(f\"Precision: {precision_score(y_valid, y_valid_pred, average='weighted')}\\n\")\n",
        "print(f\"Recall: {recall_score(y_valid, y_valid_pred, average='weighted')}\\n\")\n",
        "print(f\"F1 Score: {f1_score(y_valid, y_valid_pred, average='weighted')}\\n\")\n",
        "\n",
        "# Confusion Matrix for Validation Set\n",
        "conf_matrix_valid = confusion_matrix(y_valid, y_valid_pred)\n",
        "print(\"Confusion Matrix - Validation Set:\")\n",
        "print(conf_matrix_valid)"
      ],
      "metadata": {
        "id": "7jB4xXJe6pzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the Naive Bayes model\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "9AsRfQwWKKx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the test set\n",
        "y_test_pred = nb_model.predict(X_test)\n",
        "print(\"Gaussian Naive Bayes:\")\n",
        "\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "print(\"Test Set Evaluation:\\n\")\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred)}\\n\")\n",
        "print(f\"Precision: {precision_score(y_test, y_test_pred, average='weighted')}\\n\")\n",
        "print(f\"Recall: {recall_score(y_test, y_test_pred, average='weighted')}\\n\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_test_pred, average='weighted')}\\n\")\n",
        "\n",
        "# Confusion Matrix for Test Set\n",
        "conf_matrix_test = confusion_matrix(y_test, y_test_pred)\n",
        "print(\"Confusion Matrix - Test Set:\")\n",
        "print(conf_matrix_test, \"\\n\")\n",
        "\n",
        "# Predict on the validation set\n",
        "y_valid_pred = nb_model.predict(X_valid)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "print(\"Validation Set Evaluation:\\n\")\n",
        "print(classification_report(y_valid, y_valid_pred))\n",
        "print(f\"Accuracy: {accuracy_score(y_valid, y_valid_pred)}\\n\")\n",
        "print(f\"Precision: {precision_score(y_valid, y_valid_pred, average='weighted')}\\n\")\n",
        "print(f\"Recall: {recall_score(y_valid, y_valid_pred, average='weighted')}\\n\")\n",
        "print(f\"F1 Score: {f1_score(y_valid, y_valid_pred, average='weighted')}\\n\")\n",
        "\n",
        "# Confusion Matrix for Validation Set\n",
        "conf_matrix_valid = confusion_matrix(y_valid, y_valid_pred)\n",
        "print(\"Confusion Matrix - Validation Set:\")\n",
        "print(conf_matrix_valid)"
      ],
      "metadata": {
        "id": "4CPm60U3KRlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Gradio***"
      ],
      "metadata": {
        "id": "H17EvzFOadoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "r3HG56i8XKBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "dfochppVXHTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import string\n",
        "import warnings\n",
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/RedDragon_Reviews.csv')\n",
        "\n",
        "# Preprocess the text\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    words = word_tokenize(text)\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "df['processed_review'] = df['review'].apply(preprocess_text)\n",
        "\n",
        "# Sentiment analysis\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "df[\"Sentiment_Score\"] = df[\"processed_review\"].apply(lambda x: analyzer.polarity_scores(x)[\"compound\"])\n",
        "\n",
        "# Label encoding\n",
        "df['label_encoded'] = df['label'].apply(lambda x: 1 if x == 'AI' else 0)\n",
        "\n",
        "# Features\n",
        "features = ['Sentiment_Score']\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(3, 3))  # Use trigrams\n",
        "\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df['processed_review'])\n",
        "X_tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Apply PCA for dimensionality reduction\n",
        "pca = PCA(n_components=100)  # Adjust n_components based on your dataset\n",
        "X_pca = pca.fit_transform(X_tfidf_df)\n",
        "\n",
        "# Convert PCA components DataFrame column names to strings\n",
        "X_pca_df = pd.DataFrame(X_pca).add_prefix('pca_')\n",
        "\n",
        "# Combine PCA components with sentiment score\n",
        "X = pd.concat([df[features].reset_index(drop=True), X_pca_df.reset_index(drop=True)], axis=1)\n",
        "y = df['label_encoded']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Decision Tree model\n",
        "decision_tree_model = DecisionTreeClassifier(random_state=42)\n",
        "decision_tree_model.fit(X_train, y_train)\n",
        "\n",
        "# Function to predict if a review is AI or OR\n",
        "def predict_review(Review):\n",
        "    processed_review = preprocess_text(Review)\n",
        "    sentiment_score = analyzer.polarity_scores(processed_review)[\"compound\"]\n",
        "\n",
        "    review_tfidf = tfidf_vectorizer.transform([processed_review]).toarray()\n",
        "    review_pca = pca.transform(review_tfidf)\n",
        "\n",
        "    features = np.concatenate(([sentiment_score], review_pca[0]))\n",
        "\n",
        "    prediction = decision_tree_model.predict([features])\n",
        "\n",
        "    if prediction == 1:\n",
        "        return \"<span style='color: red;font-weight: bold;font-size: 20px;'>CAUTION!!!!<br>There is a possibility that the review might be written using an AI (Fake)</span>\"\n",
        "    else:\n",
        "        return \"<span style='color: green;font-weight: bold;font-size: 20px;'>The review is written by a customer....Original Review (Not Fake)</span>\"\n",
        "\n",
        "# Create Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=predict_review,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"html\",\n",
        "    title=\"<b>Review Authenticity Predictor</b>\",\n",
        "    description=\"<small><i>Please Note: As the model is not completely deployed/checked for overfitting, there could be scenarios where a few of the predictions might be incorrect.</i></small>\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "lDtZ5G_KQI95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Named Entity Recognition (NER)***"
      ],
      "metadata": {
        "id": "QuH7rikH8vbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk import ne_chunk, pos_tag, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tree import Tree\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "# Ensure the necessary NLTK packages are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load your dataset (ensure the correct file path is provided)\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/RedDragon_Reviews.csv')\n",
        "\n",
        "# Get the set of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.update([\n",
        "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your',\n",
        "    'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it',\n",
        "    \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this',\n",
        "    'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',\n",
        "    'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
        "    'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n",
        "    'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\n",
        "    'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most',\n",
        "    'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can',\n",
        "    'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren',\n",
        "    \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven',\n",
        "    \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\",\n",
        "    'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"\n",
        "])\n",
        "\n",
        "# Function to extract named entities using NLTK\n",
        "def get_named_entities(text):\n",
        "    words = word_tokenize(text)\n",
        "    tags = pos_tag(words)\n",
        "    chunked_nes = ne_chunk(tags)\n",
        "    named_entities = []\n",
        "    for chunk in chunked_nes:\n",
        "        if isinstance(chunk, Tree):\n",
        "            entity = \" \".join([token for token, pos in chunk.leaves()])\n",
        "            if entity.lower() not in stop_words:  # Filter out stopwords\n",
        "                named_entities.append(entity)\n",
        "    return named_entities\n",
        "\n",
        "# For \"OR\" Label\n",
        "\n",
        "# Filter the reviews based on the 'OR' label\n",
        "df_or = df[df['label'] == 'OR']\n",
        "\n",
        "# Apply the NER function to the 'review' column\n",
        "df_or['nltk_named_entities'] = df_or['review'].apply(get_named_entities)\n",
        "\n",
        "# Flatten the list of named entities\n",
        "named_entities_or = [entity for sublist in df_or['nltk_named_entities'] for entity in sublist]\n",
        "\n",
        "# Count the frequency of each named entity\n",
        "entity_counts_or = Counter(named_entities_or)\n",
        "\n",
        "# Get the top 20 most common named entities\n",
        "most_common_entities_or = entity_counts_or.most_common(20)\n",
        "\n",
        "# Extract entities and their counts for plotting without using zip\n",
        "entities_or = [entity for entity, count in most_common_entities_or]\n",
        "counts_or = [count for entity, count in most_common_entities_or]\n",
        "\n",
        "# Plotting the bar chart for OR label\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(entities_or, counts_or, color='skyblue')\n",
        "plt.xlabel('Frequency')\n",
        "plt.title('Top 20 Most Frequent Named Entities in Original Review (OR)',fontweight='bold',fontstyle='italic')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6isXIMcBtdpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For \"AI\" Label\n",
        "\n",
        "# Filter the reviews based on the 'AI' label\n",
        "df_ai = df[df['label'] == 'AI']\n",
        "\n",
        "# Apply the NER function to the 'review' column\n",
        "df_ai['nltk_named_entities'] = df_ai['review'].apply(get_named_entities)\n",
        "\n",
        "# Flatten the list of named entities\n",
        "named_entities_ai = [entity for sublist in df_ai['nltk_named_entities'] for entity in sublist]\n",
        "\n",
        "# Count the frequency of each named entity\n",
        "entity_counts_ai = Counter(named_entities_ai)\n",
        "\n",
        "# Get the top 20 most common named entities\n",
        "most_common_entities_ai = entity_counts_ai.most_common(20)\n",
        "\n",
        "# Separate the entities and their counts for plotting\n",
        "entities_ai, counts_ai = zip(*most_common_entities_ai)\n",
        "\n",
        "# Plotting the bar chart for AI label\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(entities_ai, counts_ai,color='lightgreen')\n",
        "plt.xlabel('Frequency')\n",
        "plt.title('Top 20 Most Frequent Named Entities in AI-generated reviews',fontweight='bold',fontstyle='italic')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4zfHAYaFmCUG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}